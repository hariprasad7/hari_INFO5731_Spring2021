{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_class_exercise_07.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariprasad7/hari_INFO5731_Spring2021/blob/main/In_class_exercise_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TDPxTNYO1HQ"
      },
      "source": [
        "# **The seventh in-class-exercise (20 points in total, 3/16/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZng-lV_O1HR"
      },
      "source": [
        "Question description: In the last in-class-exercise (exercise-06), you collected the titles of 100 articles about data science, natural language processing, and machine learning. The 100 article titles will be used as the text corpus of this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfh89uOuO1HS"
      },
      "source": [
        "(1) (8 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgehEU3_O1HS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "a2410e57-b065-4b01-e2fe-b53c31ac6622"
      },
      "source": [
        "#Gensim,plotting tools,logging\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "!pip install pyLDAvis\n",
        "!pip install --upgrade gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/38/6d81eff34c84c9158d3b7c846bff978ac88b0c2665548941946d3d591158/pyLDAvis-3.2.2.tar.gz (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.36.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis) (1.15.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135593 sha256=893e7b4110d04f2cc9854b3a1ffe83a40613e0f7ece29fdd37e3c3de6ddf5717\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/df/b6/97234c8446a43be05c9a8687ee0db1f1b5ade5f27729187eae\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.15 pyLDAvis-3.2.2\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/afe2315e08a38967f8a3036bbe7e38b428e9b7a90e823a83d0d49df1adf5/gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4zYudV4NzfX",
        "outputId": "0ab7044a-b2da-4cca-c5f2-29efd0658234"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')               #stopwords\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Hmjg6PgDN4Z0",
        "outputId": "91c062ba-5cb3-460d-abd4-fea80887ad62"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/Inclass6.csv\")\n",
        "df.head(100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\n                  Privacy Preserving Data Mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\n                   \\t Privacy-Preserving Dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\\nData Mining: Concepts and Techniques\\n      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\n                  The WEKA Data Mining Softw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\\n                  Efficient and Effective Cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>\\n                  Normalization of Data in D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>\\n                  Knowledge Discovery in Dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>\\nData Mining in Social Networks\\n            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>\\n                  Materialized Views in Data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>\\n                  An introduction to ROC ana...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0                                                  0\n",
              "0            0  \\n                  Privacy Preserving Data Mi...\n",
              "1            1  \\n                   \\t Privacy-Preserving Dat...\n",
              "2            2  \\nData Mining: Concepts and Techniques\\n      ...\n",
              "3            3  \\n                  The WEKA Data Mining Softw...\n",
              "4            4  \\n                  Efficient and Effective Cl...\n",
              "..         ...                                                ...\n",
              "95          95  \\n                  Normalization of Data in D...\n",
              "96          96  \\n                  Knowledge Discovery in Dat...\n",
              "97          97  \\nData Mining in Social Networks\\n            ...\n",
              "98          98  \\n                  Materialized Views in Data...\n",
              "99          99  \\n                  An introduction to ROC ana...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEOTbHjBPsMM",
        "outputId": "9fae8a86-5168-4b7e-b529-1ee79f1e7fa0"
      },
      "source": [
        "import re\n",
        "df.columns=[\"Index\", \"Title\"]\n",
        "df['Title']=df['Title'].str.lower()\n",
        "d = df['Title'].values.tolist() \n",
        "d= [re.sub(\" $\",\"\",se) for se in d]\n",
        "d = [re.sub('\\S*@\\S*\\s?', '', se) for se in d]\n",
        "d= [re.sub('\\s+', ' ', se) for se in d]\n",
        "d = [re.sub(\"\\'\", \"\", se) for se in d]\n",
        "print(d[:100])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' privacy preserving data mining ', ' privacy-preserving data mining ', ' data mining: concepts and techniques ', ' the weka data mining software: an update ', ' efficient and effective clustering methods for spatial data mining ', ' data mining: an overview from database perspective ', ' from data mining to knowledge discovery in databases. ', ' data mining approaches for intrusion detection, ', ' survey of clustering data mining techniques ', ' the elements of statistical learning -- data mining, inference, and prediction ', ' untangling text data mining ', ' fastmap: a fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets ', ' sprint: a scalable parallel classifier for data mining ', ' astrolabe: a robust and scalable technology for distributed system monitoring, management, and data mining ', ' limiting privacy breaches in privacy preserving data mining ', ' data mining ', ' graph-based data mining ', ' data mining ', ' data mining ', ' data mining for web intelligence ', ' data mining ', ' mining the network value of customers ', ' the quest data mining system ', ' sliq: a fast scalable classifier for data mining ', ' a data mining framework for building intrusion detection models. in: ', ' data mining ', ' data mining an eeg dataset with an ', ' data preparation for mining world wide web browsing patterns ', ' data mining ', ' data mining ', ' data mining ', ' uniqueness of medical data mining ', ' data mining ', ' sting: a statistical information grid approach to spatial data mining ', ' top 10 algorithms in data mining ', ' on the need for time series data mining benchmarks: a survey and empirical demonstration ', ' data mining for healthcare management ', ' efficient data mining for path traversal patterns ', ' privacy preserving data mining ', ' tools for privacy preserving distributed data mining ', ' towards parameter-free data mining ', ' mining sequential patterns ', ' why data mining primitives and ', ' extensibility in data mining systems ', ' neuronové sítì v data mining ', ' data mining of user navigation patterns ', ' theoretical framework for data mining ', ' application of data mining in bioinformatics ', ' data mining ', ' data mining ', ' web usage mining: discovery and applications of usage patterns from web data ', ' defining privacy for data mining ', ' data mining ', ' data mining ', ' data mining ', ' data mining ', ' and data mining ', ' • data mining ', ' data mining ', ' data mining ', ' data mining ', ' data-mining ', ' data mining ', ' an overview on data mining ', ' future trends in data mining ', ' organizational data mining in korea ', ' data mining techniques and applications ', ' automatic subspace clustering of high dimensional data ', ' machine learning and data mining ', ' accomplishments and challenges in literature data mining for biology ', ' data mining for direct marketing: problems and solutions ', ' mining sequential patterns: generalizations and performance improvements ', ' multi-relational data mining: an introduction ', ' reality mining: sensing complex social systems ', ' interestingness measures for data mining: a survey ', ' data mining perspective ', ' bayesian networks for data mining ', ' mining frequent patterns without candidate generation: a frequent-pattern tree approach ', ' cure: an efficient clustering algorithm for large data sets ', ' spatial data mining ', ' the development of data mining ', ' data mining with big data ', ' a demonstration of data mining ', ' scalable parallel data mining for association rules ', ' dynamic itemset counting and implication rules for market basket data ', ' an overview of temporal data mining ', ' data mining with differential privacy ', ' a characterization of data mining ', ' data mining; intrusion ', ' towards semantic data mining ', ' mining association rules between sets of items in large databases ', ' data streams: algorithms and applications ', ' active data mining. ', ' granular computing for data mining ', ' data mining: trends in . . . ', ' normalization of data in data mining ', ' knowledge discovery in data mining and massive data mining ', ' data mining in social networks ', ' materialized views in data mining ', ' an introduction to roc analysis. ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwpmW9sYTsx0"
      },
      "source": [
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import models\n",
        "def setowor(sen):\n",
        "    for sentence in sen:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "d_words = list(setowor(d))\n",
        "print(d_words[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLdyiMfBTv3E"
      },
      "source": [
        "#Bigram and Trigram models\n",
        "bi_gram = gensim.models.Phrases(d_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "tri_gram = gensim.models.Phrases(bi_gram[d_words], threshold=100)  \n",
        "bi_mod = gensim.models.phrases.Phraser(bi_gram)\n",
        "tri_mod = gensim.models.phrases.Phraser(tri_gram)\n",
        "print(tri_mod[bi_mod[d_words[1]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BqrEccnTy5c"
      },
      "source": [
        "# Defining functions for stopwords, Bigrams, Trigrams and Lemmatization\n",
        "def rem_stopwords(t):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in t]\n",
        "def makbigrams(t):\n",
        "    return [bi_mod[doc] for doc in t]\n",
        "def maktrigrams(t):\n",
        "    return [tri_mod[bi_mod[doc]] for doc in t]\n",
        "def lemmat(t, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts = []\n",
        "    for sent in t:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts\n",
        "d_wordsnostops = rem_stopwords(d_words)        # Removal of Stop Words and form Bigrams\n",
        "d_wordsbi_grams = makbigrams(d_wordsnostops)\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "d_lemma = lemmat(d_wordsbi_grams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])        #Lemmatization\n",
        "print(d_lemma[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7l1cOH0RwTH"
      },
      "source": [
        "id2word = corpora.Dictionary(d_lemma)           #Dictionary, corpus, tdf and view\n",
        "t = d_lemma\n",
        "corpus = [id2word.doc2bow(text) for text in t]\n",
        "print(corpus[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzXOHwXORy1a"
      },
      "source": [
        "res = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:]]\n",
        "LDAmodel = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=20,random_state=100,update_every=1,chunksize=100,passes=20,alpha='auto',per_word_topics=True)\n",
        "pprint(LDAmodel.print_topics())                  # Keyword in 20 topics\n",
        "docLDA = LDAmodel[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqnHe9hLR3Sn"
      },
      "source": [
        "print('\\nPerplexity: ', LDAmodel.log_perplexity(corpus))   #perplexity\n",
        "coherence_model_lda = CoherenceModel(model=LDAmodel, texts=d_lemma, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv8G_VRrR6Ei"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6EOLyobR-ml"
      },
      "source": [
        "import os     \n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      \n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       \n",
        "install_java()\n",
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwv7CmYqSH1g"
      },
      "source": [
        "def compcoherenceval(dictionary, corpus, texts, start, limit, step):\n",
        "    \n",
        "    coherenceval= []\n",
        "    modelli= []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(malletpath, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        modelli.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherenceval.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return modelli, coherenceval\n",
        "modelli, coherenceval = compcoherenceval(dictionary=id2word, corpus=corpus, texts=d_lemma, start=2, limit=40, step=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0moRyTQaSKYi"
      },
      "source": [
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(y, coherenceval)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherenceval\"), loc='best')\n",
        "plt.show()                                     #Graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpN9PmEoST_V"
      },
      "source": [
        "for l, v in zip(y, coherenceval):\n",
        "    print(\"Num of Topics =\", l, \" has Coherence Value:\", round(v, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlK_j0XaSWET"
      },
      "source": [
        "optimal_model = modelli[5]\n",
        "model_topics = optimal_model.show_topics(formatted=False)\n",
        "pprint(optimal_model.print_topics(num_words=10))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRA-bI9ASZvJ"
      },
      "source": [
        "def ftopicssentences(ldamodel=LDAmodel, corpus=corpus, texts=d):\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    \n",
        "    for i, row in enumerate(ldamodel[corpus]):           #For main topic in each document\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant Topic', 'Perc_Contribution', 'Topic Keywords']\n",
        "\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "df_topic_sents_keywords = ftopicssentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document Num', 'Dominant Topic', 'Topic Perc_Contribution', 'Keywords', 'Texts']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wSvzPDISZ2C"
      },
      "source": [
        "topicssorteddf_mallet = pd.DataFrame()                     #Top 5\n",
        "\n",
        "topicsoutdf_grpd = df_topic_sents_keywords.groupby('Dominant Topic')\n",
        "\n",
        "for i, grp in topicsoutdf_grpd:\n",
        "    topicssorteddf_mallet = pd.concat([topicssorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "   \n",
        "topicssorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "topicssorteddf_mallet.columns = ['Topic No', \"Topic Perc_Contrib\", \"Keywords\", \"Texts\"]    #Format\n",
        "\n",
        "topicssorteddf_mallet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqvqW6aO1HS"
      },
      "source": [
        "## (2) (8 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MexBWQqkO1HS"
      },
      "source": [
        "# Write your code here\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "def load_data(path,file_name):\n",
        "    doc_list = []\n",
        "    titles=[]\n",
        "    with open( os.path.join(path, file_name) ,\"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            text = line.strip()\n",
        "            doc_list.append(text)\n",
        "    print(\"Total No: of Documents:\",len(doc_list))\n",
        "    titles.append( text[0:min(len(text),100)] )\n",
        "    return doc_list,titles\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GOF_rrgShMT"
      },
      "source": [
        "def preprocess_data(doc_set):\n",
        "  \n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    p_stemmer = PorterStemmer()\n",
        "    texts = []\n",
        "    for i in doc_set:\n",
        "        raw = i.lower()   #cleaning\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        texts.append(stemmed_tokens)   #Preprocessed text\n",
        "    return texts  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjggorirShS_"
      },
      "source": [
        "def prepare_corpus(clean_doc):\n",
        "    dictionary = corpora.Dictionary(clean_doc)\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_doc]\n",
        "    return dictionary,doc_term_matrix"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVB8PVutSnpm"
      },
      "source": [
        "def create_gensim_lsa_model(clean_doc,number_of_topics,words):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(clean_doc)\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  \n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel        #LSA model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDMBt7eBSp00"
      },
      "source": [
        "def compcoherencevalues(dictionary, doc_term_matrix, clean_doc, stop, start=2, step=3):\n",
        "    coherencevalues = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=clean_doc, dictionary=dictionary, coherence='c_v')\n",
        "        coherencevalues.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherencevalues"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVHYJeELTlUl"
      },
      "source": [
        "def plot_graph(clean_doc,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(clean_doc)\n",
        "    model_list, coherencevalues = compcoherencevalues(dictionary, doc_term_matrix,clean_doc,\n",
        "                                                            stop, start, step)\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherencevalues)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence values\"), loc='best')\n",
        "    plt.show()                                   #Graph\n",
        "start,stop,step=2,14,1\n",
        "plot_graph(clean_text,start,stop,step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Zf03STO1HT"
      },
      "source": [
        "## (3) (4 points) Compare the results generated by the two topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9olvYfFCTkJ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vl36SYpO1HT"
      },
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "'''\n",
        "As we all know, LDA is used to assign a subject to text in a document. It creates a topic per document and word per topic model, both of which are modeled as Dirichlet distributions are a type of distribution. LSA is a computer program that analyzes text and creates a report. From a corpus of text, create a semantic space. In this case, LDA focuses on resolving a problem. LSA focuses on decreasing matrix dimension, while modeling focuses on solving problems. When it comes to contrasting\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}