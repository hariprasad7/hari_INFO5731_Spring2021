{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariprasad7/hari_INFO5731_Spring2021/blob/main/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JVYukOV0xXM"
      },
      "source": [
        "## The fifth In-class-exercise (2/23/2021, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z9XmhQG0xXO"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k26uGhs8GU-E",
        "outputId": "2e31afd5-3b8a-4b10-8551-e62c7fbf0bfa"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import pandas as pd\r\n",
        "def forurl(page_num):\r\n",
        "  if page_num>=10:\r\n",
        "    url=f'https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start={page_num}'\r\n",
        "  else:\r\n",
        "    url=\"https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc\"\r\n",
        "  req = requests.get(url)\r\n",
        "  soup = BeautifulSoup(req.content, 'html.parser')\r\n",
        "  soup1 = soup.find_all('div', {'class': 'pubabstract'})\r\n",
        "  list1=[]\r\n",
        "  for abstract in soup1:\r\n",
        "    list1.append(abstract.text)\r\n",
        "  return list1\r\n",
        "list2=[]\r\n",
        "for i in range(0,500,10):\r\n",
        "  list2=list2+forurl(i)\r\n",
        "df=pd.DataFrame(list2)\r\n",
        "print(df)\r\n",
        "df.to_csv('abstract.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                     0\n",
            "0    \\n                    Abstract not found\\n    ...\n",
            "1    \\n                     describe a method for s...\n",
            "2    \\n                    Scaling conditional rand...\n",
            "3    \\n                    The paper addresses the ...\n",
            "4    \\n                    In most natural language...\n",
            "..                                                 ...\n",
            "495  \\n                    ABSTRACT: Web service us...\n",
            "496  \\n                    Much of the pre-existing...\n",
            "497  \\n                     from sources with a hie...\n",
            "498  \\n                    . But learners get annoy...\n",
            "499  \\n                     personalities and facia...\n",
            "\n",
            "[500 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "e4zh0haGHSsY",
        "outputId": "74bdefea-ff08-4ce3-98b1-bbbf60243025"
      },
      "source": [
        "df.columns=[\"Abstracts\"]\r\n",
        "#Removing special characters and punctuations\r\n",
        "df['special characters and punctuations removal']=df['Abstracts'].str.replace('[!-;:,<>./?@#$%^&*_~{}]','')\r\n",
        "#Remove numbers\r\n",
        "df['Numbers removal']=df['Abstracts'].str.replace('\\d','')\r\n",
        "#Remove stopwords\r\n",
        "f=open('Stopwords.txt','r')#pulled stopwords provided\r\n",
        "a=f.readlines()\r\n",
        "list3=[]\r\n",
        "for i in a:\r\n",
        "  list3.append(i.strip())\r\n",
        "df['Stopwords removal']=df['Abstracts'].apply(lambda x: \" \".join(x for x in x.split() if x not in list3))\r\n",
        "#Lower casing\r\n",
        "df['Lower casing']=df['Abstracts'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "#stemming\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "st=PorterStemmer()\r\n",
        "df['Stemming']=df['Abstracts'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "#Lemmatization\r\n",
        "import nltk\r\n",
        "from textblob import Word\r\n",
        "nltk.download('wordnet')\r\n",
        "df['Lemmatization']=df['Abstracts'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "df.to_csv('data_clean.csv')\r\n",
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstracts</th>\n",
              "      <th>special characters and punctuations removal</th>\n",
              "      <th>Numbers removal</th>\n",
              "      <th>Stopwords removal</th>\n",
              "      <th>Lower casing</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n                    Abstract not found\\n    ...</td>\n",
              "      <td>\\n                    Abstract not found\\n    ...</td>\n",
              "      <td>\\n                    Abstract not found\\n    ...</td>\n",
              "      <td>Abstract found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>Abstract not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n                     describe a method for s...</td>\n",
              "      <td>\\n                     describe a method for s...</td>\n",
              "      <td>\\n                     describe a method for s...</td>\n",
              "      <td>describe method statistical modeling based max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>describ a method for statist model base on max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n                    Scaling conditional rand...</td>\n",
              "      <td>\\n                    Scaling conditional rand...</td>\n",
              "      <td>\\n                    Scaling conditional rand...</td>\n",
              "      <td>Scaling conditional random fields natural lang...</td>\n",
              "      <td>scaling conditional random fields for natural ...</td>\n",
              "      <td>scale condit random field for natur languag pr...</td>\n",
              "      <td>Scaling conditional random field for natural l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n                    The paper addresses the ...</td>\n",
              "      <td>\\n                    The paper addresses the ...</td>\n",
              "      <td>\\n                    The paper addresses the ...</td>\n",
              "      <td>The paper addresses issue cooperation linguist...</td>\n",
              "      <td>the paper addresses the issue of cooperation b...</td>\n",
              "      <td>the paper address the issu of cooper between l...</td>\n",
              "      <td>The paper address the issue of cooperation bet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n                    In most natural language...</td>\n",
              "      <td>\\n                    In most natural language...</td>\n",
              "      <td>\\n                    In most natural language...</td>\n",
              "      <td>In natural language processing applications, D...</td>\n",
              "      <td>in most natural language processing applicatio...</td>\n",
              "      <td>In most natur languag process applications, de...</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>\\n                    ABSTRACT: Web service us...</td>\n",
              "      <td>\\n                    ABSTRACT Web service use...</td>\n",
              "      <td>\\n                    ABSTRACT: Web service us...</td>\n",
              "      <td>ABSTRACT: Web service used Service Oriented Ar...</td>\n",
              "      <td>abstract: web service used in service oriented...</td>\n",
              "      <td>abstract: web servic use in servic orient arch...</td>\n",
              "      <td>ABSTRACT: Web service used in Service Oriented...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>\\n                    Much of the pre-existing...</td>\n",
              "      <td>\\n                    Much of the preexisting ...</td>\n",
              "      <td>\\n                    Much of the pre-existing...</td>\n",
              "      <td>Much pre-existing electronic data could harnes...</td>\n",
              "      <td>much of the pre-existing electronic data that ...</td>\n",
              "      <td>much of the pre-exist electron data that could...</td>\n",
              "      <td>Much of the pre-existing electronic data that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>\\n                     from sources with a hie...</td>\n",
              "      <td>\\n                     from sources with a hie...</td>\n",
              "      <td>\\n                     from sources with a hie...</td>\n",
              "      <td>sources hierarchical structure. The resulting ...</td>\n",
              "      <td>from sources with a hierarchical structure. th...</td>\n",
              "      <td>from sourc with a hierarch structure. the resu...</td>\n",
              "      <td>from source with a hierarchical structure. The...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>\\n                    . But learners get annoy...</td>\n",
              "      <td>\\n                     But learners get annoye...</td>\n",
              "      <td>\\n                    . But learners get annoy...</td>\n",
              "      <td>. But learners get annoyed language rules old ...</td>\n",
              "      <td>. but learners get annoyed with the language r...</td>\n",
              "      <td>. but learner get annoy with the languag rule ...</td>\n",
              "      <td>. But learner get annoyed with the language ru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>\\n                     personalities and facia...</td>\n",
              "      <td>\\n                     personalities and facia...</td>\n",
              "      <td>\\n                     personalities and facia...</td>\n",
              "      <td>personalities facial emotions avatars based cu...</td>\n",
              "      <td>personalities and facial emotions of avatars b...</td>\n",
              "      <td>person and facial emot of avatar base on cultu...</td>\n",
              "      <td>personality and facial emotion of avatar based...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstracts  ...                                      Lemmatization\n",
              "0    \\n                    Abstract not found\\n    ...  ...                                 Abstract not found\n",
              "1    \\n                     describe a method for s...  ...  describe a method for statistical modeling bas...\n",
              "2    \\n                    Scaling conditional rand...  ...  Scaling conditional random field for natural l...\n",
              "3    \\n                    The paper addresses the ...  ...  The paper address the issue of cooperation bet...\n",
              "4    \\n                    In most natural language...  ...  In most natural language processing applicatio...\n",
              "..                                                 ...  ...                                                ...\n",
              "495  \\n                    ABSTRACT: Web service us...  ...  ABSTRACT: Web service used in Service Oriented...\n",
              "496  \\n                    Much of the pre-existing...  ...  Much of the pre-existing electronic data that ...\n",
              "497  \\n                     from sources with a hie...  ...  from source with a hierarchical structure. The...\n",
              "498  \\n                    . But learners get annoy...  ...  . But learner get annoyed with the language ru...\n",
              "499  \\n                     personalities and facia...  ...  personality and facial emotion of avatar based...\n",
              "\n",
              "[500 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qk3PDH-IyjO"
      },
      "source": [
        "#compute the tf idf and tf-idf values for the starting word in the text\r\n",
        "#computeTF\r\n",
        "def TF(sentence):\r\n",
        "  word = sentence.split(\" \")\r\n",
        "  value = len(set(word))\r\n",
        "  return words.count(word[0])/value\r\n",
        "  pass\r\n",
        "#compute IDF\r\n",
        "import math \r\n",
        "def IDF(sentence):\r\n",
        "  words = sentence.lower().split(\" \")\r\n",
        "  res = words.count(words[0])\r\n",
        "  return math.log(len(words)/res, 10)\r\n",
        "  pass\r\n",
        "df[\"word used for TF-IDF\"]=df[\"Lemmatization\"].apply(lambda x : x.split(\" \")[0])\r\n",
        "df[\"tf\"]=df[\"Lemmatization\"].apply(lambda x : round(TF(x),2))\r\n",
        "df[\"idf\"]=df[\"Lemmatization\"].apply(lambda x : round(IDF(x),2))\r\n",
        "df[\"tf-idf\"]=df1[\"tf\"]*df[\"idf\"]\r\n",
        "df"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gju3wMdI0xXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f429d82-4cba-409d-ed27-25600e76fdfd"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('universal_tagset')\r\n",
        "from textblob import TextBlob\r\n",
        "nltk.download('punkt')\r\n",
        "f=open(\"movies_list.csv\",\"r\")\r\n",
        "df=pd.DataFrame(f)\r\n",
        "df.columns=['ForEx5']\r\n",
        "df['ForEx5']=df['ForEx5'].apply(lambda x: TextBlob(x).words)\r\n",
        "dic1={}\r\n",
        "for dff in df['ForEx5']:\r\n",
        "  op=nltk.pos_tag(dff, tagset='universal')\r\n",
        "  dic1.update(dict(op))\r\n",
        "print(dic1)\r\n",
        "'''\r\n",
        "def count(POS):\r\n",
        "  pos='{}'.format(POS)\r\n",
        "  if v==pos:\r\n",
        "    '{}_count'.format(POS)='{}_count'.format(POS)+1\r\n",
        "  return c\r\n",
        "for k,v in dic1.items():\r\n",
        "  print(count('NOUN'))\r\n",
        "'''\r\n",
        "Noun_count=0\r\n",
        "Verb_count=0\r\n",
        "Adjective_count=0\r\n",
        "Adverb_count=0\r\n",
        "for key, value in dic1.items():\r\n",
        "  if value=='NOUN':\r\n",
        "    Noun_count+=1\r\n",
        "  if value=='VERB':\r\n",
        "    Verb_count+=1\r\n",
        "  if value=='ADJ':\r\n",
        "    Adjective_count+=1\r\n",
        "  if value=='ADV':\r\n",
        "    Adverb_count+=1\r\n",
        "print('Total number of Nouns: {}\\nTotal number of Verbs: {}\\nTotal number of Adjectives: {}\\nTotal number of Adverbs: {}\\n'.format(Noun_count,Verb_count,Adjective_count,Adverb_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'movie_name': 'NOUN', '0': 'NUM', 'Dead': 'ADJ', 'Man': 'NOUN', '1': 'NUM', 'All': 'DET', 'Good': 'ADJ', 'Things': 'NOUN', '2': 'NUM', 'Brick': 'NOUN', '3': 'NUM', 'Breakfast': 'NOUN', 'at': 'ADP', 'Tiffany': 'NOUN', \"'s\": 'PRT', '4': 'NUM', 'Billy': 'NOUN', 'Elliot': 'NOUN', '5': 'NUM', 'Buried': 'NOUN', '6': 'NUM', 'The': 'DET', 'Crying': 'NOUN', 'Game': 'NOUN', '7': 'NUM', 'Case': 'NOUN', '39': 'NUM', '8': 'NUM', 'Crow': 'NOUN', '9': 'NUM', 'Downfall': 'NOUN', '10': 'NUM', 'Daddy': 'NOUN', 'Day': 'NOUN', 'Care': 'NOUN', '11,8MM': 'NUM', '12': 'NUM', 'Following': 'VERB', '13': 'NUM', 'Will': 'VERB', 'Hunting': 'VERB', '14': 'NUM', 'Gosford': 'NOUN', 'Park': 'NOUN', '15': 'NUM', 'Heathers': 'NOUN', '16': 'NUM', 'Insidious': 'ADJ', '17': 'NUM', 'Joneses': 'NOUN', '18': 'NUM', 'Kite': 'NOUN', 'Runner': 'NOUN', '19': 'NUM', 'Let': 'VERB', 'the': 'DET', 'Right': 'NOUN', 'One': 'NUM', 'In': 'ADP', '20': 'NUM', 'Last': 'ADJ', 'of': 'ADP', 'Mohicans': 'NOUN', '21': 'NUM', 'Emperor': 'NOUN', '22': 'NUM', 'Limitless': 'NOUN', '23': 'NUM', 'Morning': 'NOUN', 'Glory': 'NOUN', '24': 'NUM', 'Meet': 'NOUN', 'Parents': 'NOUN', '25': 'NUM', 'Who': 'NOUN', 'Was': 'VERB', \"n't\": 'ADV', 'There': 'DET', '26': 'NUM', 'Memento': 'NOUN', '27': 'NUM', 'Motorcycle': 'NOUN', 'Diaries': 'NOUN', '28': 'NUM', 'Mary': 'NOUN', 'and': 'CONJ', 'Max': 'NOUN', '29': 'NUM', 'Paper': 'NOUN', '30': 'NUM', 'Secret': 'NOUN', 'NIMH': 'NOUN', '31': 'NUM', 'Time': 'VERB', 'Bandits': 'NOUN', '32': 'NUM', 'True': 'ADJ', 'Grit': 'NOUN', '33': 'NUM', 'Tell': 'NOUN', 'No': 'NOUN', '34': 'NUM', 'What': 'PRON', 'Dreams': 'NOUN', 'May': 'NOUN', 'Come': 'VERB', '35': 'NUM', 'Wild': 'NOUN', 'Target': 'NOUN', '36': 'NUM', 'This': 'DET', 'Is': 'VERB', 'England': 'NOUN', '37': 'NUM', 'Dorian': 'ADJ', 'Gray': 'NOUN', '38': 'NUM', 'Rabbit': 'NOUN', 'Hole': 'NOUN', 'Thirteenth': 'NOUN', 'Floor': 'NOUN', '40': 'NUM', 'Punch-Drunk': 'ADJ', 'Love': 'NOUN', '41': 'NUM', 'Lincoln': 'NOUN', 'Lawyer': 'NOUN', '42': 'NUM', 'Finding': 'NOUN', 'Forrester': 'NOUN', '43': 'NUM', 'Waiting': 'NOUN', 'for': 'ADP', 'Forever': 'NOUN', '44': 'NUM', 'Adventureland': 'NOUN', '45': 'NUM', 'Borrowers': 'NOUN', '46': 'NUM', 'Broken': 'NOUN', 'Flowers': 'NOUN', '47': 'NUM', 'Big': 'NOUN', '48': 'NUM', 'Capote': 'NOUN', '49': 'NUM', 'Conspirator': 'NOUN', '50': 'NUM', 'Captain': 'NOUN', 'America': 'NOUN', 'First': 'ADJ', 'Avenger': 'NOUN', '51': 'NUM', 'Death': 'NOUN', 'a': 'DET', 'Funeral': 'NOUN', '52': 'NUM', 'Do': 'VERB', 'Be': 'VERB', 'Afraid': 'NOUN', 'Dark': 'NOUN', '53': 'NUM', 'Donnie': 'NOUN', 'Darko': 'NOUN', '54': 'NUM', 'Emma': 'NOUN', '55': 'NUM', 'Everybody': 'NOUN', 'Fine': 'NOUN', '56': 'NUM', 'Exit': 'NOUN', 'Through': 'ADP', 'Gift': 'NOUN', 'Shop': 'NOUN', '57': 'NUM', 'Eternal': 'ADJ', 'Sunshine': 'NOUN', 'Spotless': 'NOUN', 'Mind': 'NOUN', '58': 'NUM', 'Everything': 'NOUN', 'Must': 'NOUN', 'Go': 'NOUN', '59': 'NUM', 'Faculty': 'NOUN', '60': 'NUM', 'A': 'DET', 'Fish': 'NOUN', 'Called': 'VERB', 'Wanda': 'NOUN', '61': 'NUM', 'FernGully': 'ADV', 'Rainforest': 'NOUN', '62': 'NUM', 'Glengarry': 'NOUN', 'Glen': 'NOUN', 'Ross': 'NOUN', '63': 'NUM', 'Girl': 'NOUN', 'with': 'ADP', 'Dragon': 'NOUN', 'Tattoo': 'NOUN', '64': 'NUM', 'Heavenly': 'NOUN', 'Creatures': 'NOUN', '65': 'NUM', 'Jumanji': 'NOUN', '66': 'NUM', 'Joe': 'NOUN', 'Dirt': 'NOUN', '67': 'NUM', 'Kramer': 'NOUN', 'vs': 'NOUN', '68': 'NUM', 'Little': 'ADJ', 'Bit': 'NOUN', 'Heaven': 'NOUN', '69': 'NUM', 'Before': 'ADP', 'Devil': 'NOUN', 'Knows': 'NOUN', 'You': 'PRON', \"'re\": 'VERB', '70': 'NUM', 'Daydream': 'NOUN', 'Nation': 'NOUN', '71': 'NUM', '72': 'NUM', 'Cruel': 'NOUN', 'Intentions': 'NOUN', '73': 'NUM', 'League': 'NOUN', 'Their': 'NOUN', 'Own': 'NOUN', '74': 'NUM', 'Lost': 'NOUN', 'in': 'ADP', 'Translation': 'NOUN', '75': 'NUM', 'Legends': 'NOUN', 'Fall': 'NOUN', '76': 'NUM', 'Monsters': 'NOUN', '77': 'NUM', 'Mr': 'NOUN', 'Deeds': 'NOUN', '78': 'NUM', 'Malena': 'NOUN', '79': 'NUM', 'Matilda': 'NOUN', '80': 'NUM', 'Marathon': 'NOUN', '81': 'NUM', 'Black': 'NOUN', '82': 'NUM', 'Midnight': 'NOUN', 'Cowboy': 'NOUN', '83': 'NUM', 'My': 'NOUN', 'Best': 'NOUN', 'Friend': 'NOUN', 'Wedding': 'NOUN', '84': 'NUM', 'Next': 'ADJ', 'Three': 'NOUN', 'Days': 'NOUN', '85': 'NUM', 'Passion': 'NOUN', 'Christ': 'NOUN', '86': 'NUM', 'Rango': 'NOUN', '87': 'NUM', 'Rum': 'NOUN', 'Diary': 'NOUN', '88': 'NUM', 'Raising': 'VERB', 'Arizona': 'NOUN', '89': 'NUM', 'Rubber': 'NOUN', '90': 'NUM', 'Super': 'NOUN', '91': 'NUM', 'Skeleton': 'NOUN', 'Key': 'NOUN', '92': 'NUM', 'Steel': 'NOUN', 'Magnolias': 'NOUN', '93': 'NUM', 'Sleepless': 'NOUN', 'Seattle': 'NOUN', '94': 'NUM', 'Sling': 'VERB', 'Blade': 'NOUN', '95': 'NUM', 'Small': 'ADJ', 'Soldiers': 'NOUN', '96': 'NUM', 'Stuart': 'NOUN', '97': 'NUM', 'She': 'PRON', 'That': 'DET', '98': 'NUM', 'Trainspotting': 'VERB', '99': 'NUM', 'Thor': 'NOUN', '100': 'NUM', 'Tale': 'NOUN', 'Two': 'NUM', 'Sisters': 'NOUN', '101': 'NUM', 'Despereaux': 'NOUN', '102': 'NUM', 'Eating': 'VERB', 'Gilbert': 'NOUN', 'Grape': 'NOUN', '103': 'NUM', 'Albert': 'NOUN', 'Nobbs': 'NOUN', '104': 'NUM', 'Fatal': 'ADJ', 'Attraction': 'NOUN', '105': 'NUM', '106': 'NUM', 'Hachi': 'NOUN', 'Dog': 'NOUN', '107': 'NUM', 'Wave': 'NOUN', '108': 'NUM', 'Pact': 'NOUN', '109': 'NUM', 'Butter': 'NOUN', '110': 'NUM', 'Battle': 'NOUN', 'Royale': 'NOUN', '111': 'NUM', 'Pianist': 'NOUN', '112': 'NUM', 'Raven': 'NOUN', '113': 'NUM', 'Loop': 'NOUN', '114,8½': 'NUM', '115,101': 'NUM', 'Dalmatians': 'NOUN', '116': 'NUM', 'Adaptation': 'NOUN', '117': 'NUM', 'Adventures': 'NOUN', 'Tintin': 'NOUN', '118': 'NUM', 'Alice': 'NOUN', 'Wonderland': 'NOUN', '119': 'NUM', 'An': 'DET', 'American': 'ADJ', 'Crime': 'NOUN', '120': 'NUM', 'Werewolf': 'NOUN', 'London': 'NOUN', '121': 'NUM', 'Artist': 'NOUN', '122': 'NUM', 'AristoCats': 'NOUN', '123': 'NUM', 'As': 'ADV', 'as': 'ADP', 'It': 'PRON', 'Gets': 'VERB', '124': 'NUM', 'Atlas': 'NOUN', 'Shrugged': 'NOUN', 'Part': 'NOUN', 'I': 'PRON', '125': 'NUM', 'Barton': 'NOUN', 'Fink': 'NOUN', '126': 'NUM', 'Basic': 'NOUN', 'Instinct': 'NOUN', '127': 'NUM', 'Bernie': 'NOUN', '128': 'NUM', 'Beverly': 'NOUN', 'Hills': 'NOUN', 'Cop': 'NOUN', '129': 'NUM', 'Bicycle': 'NOUN', 'Thieves': 'NOUN', '130': 'NUM', 'Bottle': 'NOUN', 'Rocket': 'NOUN', '131': 'NUM', 'Boy': 'NOUN', 'Striped': 'NOUN', 'Pajamas': 'NOUN', '132': 'NUM', 'Bronx': 'NOUN', '133': 'NUM', 'Bully': 'NOUN', '134': 'NUM', 'Casa': 'NOUN', 'de': 'X', 'mi': 'X', 'Padre': 'NOUN', '135': 'NUM', 'Charade': 'NOUN', '136': 'NUM', 'Charlie': 'NOUN', 'Bartlett': 'NOUN', '137': 'NUM', 'Children': 'NOUN', 'Corn': 'NOUN', '138': 'NUM', 'Clue': 'NOUN', '139': 'NUM', 'Clueless': 'NOUN', '140': 'NUM', 'Company': 'NOUN', 'Men': 'NOUN', '141': 'NUM', 'Coming': 'VERB', 'to': 'PRT', '142': 'NUM', '143': 'NUM', 'Deep': 'ADJ', 'Impact': 'NOUN', '144': 'NUM', 'Double': 'ADJ', 'Jeopardy': 'NOUN', '145': 'NUM', 'Drive': 'VERB', '146': 'NUM', 'Dumbo': 'NOUN', '147': 'NUM', 'Explorers': 'NOUN', '148': 'NUM', 'Extract': 'NOUN', '149': 'NUM', 'Faces': 'NOUN', 'Crowd': 'NOUN', '150': 'NUM', 'Ferris': 'NOUN', 'Bueller': 'NOUN', 'Off': 'NOUN', '151': 'NUM', 'Jin': 'NOUN', 'ling': 'VERB', 'shi': 'ADJ', 'san': 'ADJ', 'chai': 'NOUN', '152': 'NUM', 'Forgotten': 'NOUN', '153': 'NUM', 'For': 'ADP', '154': 'NUM', 'Fourth': 'NOUN', 'Kind': 'NOUN', '155': 'NUM', 'Fox': 'NOUN', 'Hound': 'NOUN', '156': 'NUM', 'Frighteners': 'NOUN', '157': 'NUM', 'Gandhi': 'NOUN', '158': 'NUM', 'Ghost': 'NOUN', '159': 'NUM', 'Giant': 'NOUN', 'Mechanical': 'NOUN', '160': 'NUM', '161': 'NUM', 'Played': 'VERB', 'Fire': 'NOUN', '162': 'NUM', '163': 'NUM', 'Graduate': 'NOUN', '164': 'NUM', 'Grapes': 'NOUN', 'Wrath': 'NOUN', '165': 'NUM', 'Great': 'NOUN', 'Gatsby': 'NOUN', '166': 'NUM', 'Mouse': 'NOUN', 'Detective': 'NOUN', '167': 'NUM', 'Butch': 'NOUN', 'Cassidy': 'NOUN', 'Sundance': 'NOUN', 'Kid': 'NOUN', '168': 'NUM', 'Happy': 'ADJ', 'Gilmore': 'NOUN', '169': 'NUM', '170': 'NUM', 'Hotel': 'NOUN', 'Rwanda': 'NOUN', '171': 'NUM', 'Hours': 'NOUN', '172': 'NUM', 'Hugo': 'NOUN', '173': 'NUM', 'Identity': 'NOUN', '174': 'NUM', 'Ip': 'NOUN', '175': 'NUM', 'Innkeepers': 'NOUN', '176': 'NUM', 'James': 'NOUN', 'Peach': 'NOUN', '177': 'NUM', 'Jeff': 'NOUN', 'Lives': 'VERB', 'Home': 'NOUN', '178': 'NUM', 'Keith': 'NOUN', '179': 'NUM', 'Bike': 'NOUN', '180': 'NUM', 'King': 'NOUN', 'Speech': 'NOUN', '181': 'NUM', 'Lars': 'NOUN', 'Real': 'NOUN', '182': 'NUM', 'Life': 'NOUN', 'Beautiful': 'NOUN', '183': 'NUM', 'Like': 'ADP', 'Crazy': 'NOUN', '184': 'NUM', 'Lonesome': 'NOUN', 'Dove': 'NOUN', '185': 'NUM', 'Lorax': 'NOUN', '186': 'NUM', '187': 'NUM', 'Machine': 'NOUN', 'Gun': 'NOUN', 'Preacher': 'NOUN', '188': 'NUM', 'Machinist': 'NOUN', '189': 'NUM', 'Magic': 'NOUN', 'Belle': 'NOUN', 'Isle': 'NOUN', '190': 'NUM', 'Manhattan': 'NOUN', '191': 'NUM', 'Iron': 'NOUN', 'Mask': 'NOUN', '192': 'NUM', 'Miller': 'NOUN', 'Crossing': 'NOUN', '193': 'NUM', 'Mirror': 'NOUN', '194': 'NUM', 'Mona': 'NOUN', 'Lisa': 'NOUN', 'Smile': 'NOUN', '195': 'NUM', 'Mulholland': 'NOUN', 'Dr': 'NOUN', '196': 'NUM', 'Night': 'NOUN', 'Roxbury': 'NOUN', '197': 'NUM', 'Nightmare': 'NOUN', 'Christmas': 'NOUN', '198': 'NUM', 'Nowhere': 'NOUN', '199': 'NUM', 'Nutty': 'NOUN', 'Professor': 'NOUN', '200': 'NUM', 'Omen': 'NOUN', '201': 'NUM', 'Once': 'ADV', 'Upon': 'ADP', 'Mexico': 'NOUN', '202': 'NUM', 'Ordinary': 'ADJ', 'People': 'NOUN', '203': 'NUM', 'Out': 'NOUN', 'Sight': 'NOUN', '204': 'NUM', 'Planes': 'NOUN', 'Trains': 'NOUN', 'Automobiles': 'NOUN', '205': 'NUM', 'Platoon': 'NOUN', '206': 'NUM', 'Pocahontas': 'NOUN', '207': 'NUM', 'Primer': 'NOUN', '208': 'NUM', 'Producers': 'NOUN', '209': 'NUM', 'Psycho': 'NOUN', '210': 'NUM', 'Rat': 'NOUN', 'Race': 'NOUN', '211': 'NUM', 'Red': 'NOUN', 'Lights': 'NOUN', '212': 'NUM', 'Rescuers': 'NOUN', 'Down': 'NOUN', 'Under': 'ADP', '213': 'NUM', 'Romeo': 'NOUN', 'Juliet': 'NOUN', '214': 'NUM', 'Room': 'NOUN', 'View': 'NOUN', '215': 'NUM', 'Rosemary': 'NOUN', 'Baby': 'NOUN', '216': 'NUM', 'Safety': 'NOUN', 'Not': 'ADV', 'Guaranteed': 'VERB', '217': 'NUM', 'Serenity': 'NOUN', '218': 'NUM', 'Shakespeare': 'NOUN', '219': 'NUM', 'Slap': 'NOUN', 'Shot': 'NOUN', '220': 'NUM', '221': 'NUM', 'Stake': 'NOUN', 'Land': 'NOUN', '222': 'NUM', 'Stand': 'NOUN', '223': 'NUM', '224': 'NUM', 'Switch': 'NOUN', '225': 'NUM', 'Tall': 'NOUN', '226': 'NUM', 'Teen': 'NOUN', 'Wolf': 'NOUN', '227': 'NUM', 'Terms': 'NOUN', 'Endearment': 'NOUN', '228': 'NUM', 'Thing': 'NOUN', '229': 'NUM', 'Spinal': 'ADJ', 'Tap': 'NOUN', '230': 'NUM', 'Tinker': 'NOUN', 'Bell': 'NOUN', '231': 'NUM', 'Fairy': 'NOUN', 'Rescue': 'NOUN', '232': 'NUM', 'Wings': 'NOUN', '233': 'NUM', 'Tommy': 'NOUN', '234': 'NUM', 'Town': 'NOUN', '235': 'NUM', 'Trading': 'NOUN', 'Places': 'NOUN', '236': 'NUM', 'Traffic': 'NOUN', '237': 'NUM', 'Transformers': 'NOUN', 'Moon': 'NOUN', '238': 'NUM', 'Treasure': 'NOUN', 'Planet': 'NOUN', '239': 'NUM', 'Tremors': 'NOUN', '240': 'NUM', 'Untouchables': 'NOUN', '241': 'NUM', 'Vanilla': 'NOUN', 'Sky': 'NOUN', '242': 'NUM', 'Volcano': 'NOUN', '243': 'NUM', 'Planner': 'NOUN', '244': 'NUM', 'World': 'NOUN', 'Trade': 'NOUN', 'Center': 'NOUN', '245': 'NUM', 'Young': 'NOUN', 'Adult': 'NOUN', '246': 'NUM', '247': 'NUM', 'Way': 'NOUN', '248': 'NUM', 'Of': 'ADP', 'Mice': 'NOUN', '249': 'NUM', 'We': 'PRON', 'Were': 'VERB', '250': 'NUM', 'Accidents': 'NOUN', 'Happen': 'VERB', '251': 'NUM', 'Basquiat': 'NOUN', '252': 'NUM', 'Barry': 'NOUN', 'Munday': 'NOUN', '253': 'NUM', 'Shock': 'NOUN', '254': 'NUM', 'Boys': 'NOUN', 'Are': 'VERB', 'Back': 'ADV', '255': 'NUM', 'Girls': 'NOUN', '256': 'NUM', 'Camille': 'NOUN', '257': 'NUM', 'Color': 'NOUN', 'Me': 'NOUN', 'Kubrick': 'NOUN', '258': 'NUM', '259': 'NUM', 'Creation': 'NOUN', '260': 'NUM', 'Dear': 'NOUN', 'Lemon': 'NOUN', 'Lima': 'NOUN', '261': 'NUM', 'Flawless': 'NOUN', '262': 'NUM', 'Finder': 'NOUN', 'Fee': 'NOUN', '263': 'NUM', 'Flirting': 'VERB', 'Disaster': 'NOUN', '264': 'NUM', 'Son': 'NOUN', '265': 'NUM', 'Golden': 'NOUN', 'Child': 'NOUN', '266': 'NUM', 'Grifters': 'NOUN', '267': 'NUM', '268': 'NUM', 'Buddha': 'NOUN', '269': 'NUM', 'Voice': 'NOUN', '270': 'NUM', 'Her': 'NOUN', 'Eyes': 'NOUN', '271': 'NUM', 'Help': 'NOUN', '272': 'NUM', 'Long': 'NOUN', 'Walk': 'NOUN', '273': 'NUM', 'City': 'NOUN', '274': 'NUM', 'Five': 'NUM', 'Girlfriends': 'NOUN', '275': 'NUM', 'Name': 'NOUN', 'Nobody': 'NOUN', '276': 'NUM', 'Microcosmos': 'NOUN', '277': 'NUM', 'Nines': 'NOUN', '278': 'NUM', 'Train': 'NOUN', '279': 'NUM', '280': 'NUM', 'Outsourced': 'VERB', '281': 'NUM', 'Week': 'NOUN', '282': 'NUM', 'Primary': 'ADJ', 'Colors': 'NOUN', '283': 'NUM', '284': 'NUM', '285': 'NUM', 'Plunkett': 'NOUN', 'Macleane': 'NOUN', '286': 'NUM', 'Robinson': 'NOUN', 'Crusoe': 'NOUN', '287': 'NUM', 'Smoke': 'NOUN', '288': 'NUM', 'Sweet': 'NOUN', '289': 'NUM', '290': 'NUM', 'Snow': 'NOUN', 'White': 'NOUN', 'Terror': 'NOUN', '291': 'NUM', 'Shout': 'NOUN', '292': 'NUM', 'Ten': 'NUM', 'Inch': 'NOUN', 'Hero': 'NOUN', '293': 'NUM', 'Timer': 'NOUN', '294': 'NUM', 'Vanishing': 'VERB', '295': 'NUM', 'Velvet': 'NOUN', 'Goldmine': 'NOUN', '296': 'NUM', 'War': 'NOUN', '297': 'NUM', 'Yellow': 'NOUN', 'Handkerchief': 'NOUN', '298': 'NUM', 'Comet': 'NOUN', '299': 'NUM', 'Buck': 'NOUN', '300': 'NUM', 'Animal': 'NOUN', 'Farm': 'NOUN', '301': 'NUM', 'Nirgendwo': 'NOUN', 'Afrika': 'NOUN', '302': 'NUM', 'Griff': 'NOUN', 'Invisible': 'ADJ', '303': 'NUM', 'Mousehunt': 'NOUN', '304': 'NUM', 'He': 'PRON', 'Quiet': 'ADJ', '305': 'NUM', 'Beguiled': 'NOUN', '306': 'NUM', '307': 'NUM', 'English': 'NOUN', '308': 'NUM', 'Stallion': 'NOUN', '309': 'NUM', 'Salvation': 'NOUN', '310': 'NUM', 'Decoy': 'NOUN', 'Bride': 'NOUN', '311': 'NUM', 'Eight': 'NOUN', '312': 'NUM', 'Earth': 'NOUN', 'Easy': 'ADJ', '313': 'NUM', 'Attic': 'ADJ', '314': 'NUM', 'Woman': 'NOUN', '315': 'NUM', '316': 'NUM', 'High': 'NOUN', 'Anxiety': 'NOUN', '317': 'NUM', 'Texas': 'NOUN', '318': 'NUM', 'Into': 'NOUN', 'West': 'NOUN', '319': 'NUM', 'Jane': 'NOUN', 'Eyre': 'NOUN', '320': 'NUM', 'California': 'NOUN', '321': 'NUM', 'Dogs': 'NOUN', '322': 'NUM', '323': 'NUM', 'Fork': 'NOUN', 'Road': 'NOUN', '324': 'NUM', 'Ramen': 'NOUN', '325': 'NUM', 'Lucky': 'ADJ', '326': 'NUM', 'Fell': 'NOUN', '327': 'NUM', 'Mighty': 'NOUN', '328': 'NUM', 'Mrs': 'NOUN', 'Brown': 'NOUN', '329': 'NUM', 'on': 'ADP', 'Wire': 'NOUN', '330': 'NUM', 'Marvin': 'NOUN', '331': 'NUM', 'Mermaids': 'NOUN', '332': 'NUM', 'Muse': 'NOUN', '333': 'NUM', 'Miss': 'NOUN', 'Minoes': 'NOUN', '334': 'NUM', 'Without': 'ADP', 'Daughter': 'NOUN', '335': 'NUM', 'Neverwas': 'NOUN', '336': 'NUM', 'Ondine': 'NOUN', '337': 'NUM', 'Outside': 'ADJ', 'Providence': 'NOUN', '338': 'NUM', '339': 'NUM', 'Project': 'NOUN', 'X': 'NOUN', '340': 'NUM', 'Parking': 'NOUN', 'Lot': 'NOUN', 'Movie': 'NOUN', '341': 'NUM', 'Peggy': 'NOUN', 'Sue': 'NOUN', 'Got': 'NOUN', 'Married': 'NOUN', '342': 'NUM', 'Pirates': 'NOUN', 'Penzance': 'NOUN', '343': 'NUM', 'Balloon': 'NOUN', '344': 'NUM', 'Rare': 'ADJ', 'Exports': 'NOUN', '345': 'NUM', 'Regarding': 'VERB', 'Henry': 'NOUN', '346': 'NUM', '347': 'NUM', '348': 'NUM', '349': 'NUM', 'Stoning': 'NOUN', 'Soraya': 'NOUN', 'M': 'NOUN', '350': 'NUM', 'Swimming': 'VERB', 'Pool': 'NOUN', '351': 'NUM', 'Shutter': 'NOUN', '352': 'NUM', 'Kells': 'NOUN', '353': 'NUM', 'Moonacre': 'NOUN', '354': 'NUM', 'Ira': 'NOUN', 'Abby': 'NOUN', '355': 'NUM', 'Peter': 'NOUN', 'Vandy': 'NOUN', '356': 'NUM', 'Paris': 'NOUN', '357': 'NUM', 'Missing': 'VERB', '358': 'NUM', 'Teaching': 'VERB', 'Tingle': 'NOUN', '359': 'NUM', 'Denver': 'NOUN', 'When': 'ADV', '360': 'NUM', 'Transsiberian': 'ADJ', '361': 'NUM', 'Too': 'NOUN', 'Die': 'VERB', '362': 'NUM', '10th': 'NUM', 'Kingdom': 'NOUN', '363': 'NUM', 'Panique': 'NOUN', 'au': 'ADJ', 'village': 'NOUN', '364': 'NUM', 'Weather': 'NOUN', 'Underground': 'NOUN', '365': 'NUM', 'Hill': 'NOUN', '366': 'NUM', 'Vanity': 'NOUN', 'Fair': 'NOUN', '367': 'NUM', 'Very': 'ADV', 'Bad': 'NOUN', '368': 'NUM', 'Diminished': 'NOUN', 'Capacity': 'NOUN', '369': 'NUM', 'Bits': 'NOUN', '370': 'NUM', 'Skateland': 'NOUN', '371': 'NUM', 'Frances': 'NOUN', '372': 'NUM', 'Far': 'NOUN', 'from': 'ADP', '373': 'NUM', 'Agnes': 'NOUN', 'Browne': 'NOUN', '374': 'NUM', 'Bag': 'NOUN', 'Bones': 'NOUN', '375': 'NUM', 'Toast': 'NOUN', '376': 'NUM', 'Une': 'NOUN', 'vie': 'X', 'chat': 'X', '377': 'NUM', 'From': 'ADJ', '378': 'NUM', 'Humboldt': 'NOUN', 'County': 'NOUN', '379': 'NUM', 'Baron': 'NOUN', '380': 'NUM', 'License': 'NOUN', '381': 'NUM', 'Buckaroo': 'NOUN', 'Banzai': 'NOUN', 'Across': 'ADP', '8th': 'NUM', 'Dimension': 'NOUN', '382': 'NUM', 'Castle': 'NOUN', '383': 'NUM', 'Look': 'VERB', '384': 'NUM', 'Vicious': 'NOUN', '385': 'NUM', 'Take': 'NOUN', '386': 'NUM', '387,48': 'NUM', 'Hrs': 'NOUN', '388': 'NUM', 'African': 'NOUN', 'Queen': 'NOUN', '389': 'NUM', 'Airheads': 'NOUN', '390': 'NUM', 'Looking': 'NOUN', 'Glass': 'NOUN', '391': 'NUM', 'Alphabet': 'NOUN', 'Killer': 'NOUN', '392': 'NUM', 'Hammers': 'NOUN', '393': 'NUM', 'Barbarella': 'NOUN', '394': 'NUM', 'Whorehouse': 'NOUN', '395': 'NUM', 'Beyond': 'NOUN', 'Silence': 'NOUN', '396': 'NUM', 'Biloxi': 'NOUN', 'Blues': 'NOUN', '397': 'NUM', 'Blow': 'NOUN', 'Dry': 'NOUN', '398': 'NUM', 'Breakdown': 'NOUN', '399': 'NUM', 'Caller': 'NOUN', '400': 'NUM', 'Canterville': 'NOUN', '401': 'NUM', 'Cat': 'NOUN', 'Meow': 'NOUN', '402': 'NUM', 'Chain': 'NOUN', 'Reaction': 'NOUN', '403': 'NUM', 'Cold': 'NOUN', 'Comfort': 'NOUN', '404': 'NUM', 'Creator': 'NOUN', '405': 'NUM', 'Cul-de-sac': 'ADJ', '406': 'NUM', 'Darkness': 'NOUN', 'Falls': 'NOUN', '407': 'NUM', 'Salesman': 'NOUN', '408': 'NUM', 'Digging': 'VERB', 'China': 'NOUN', '409': 'NUM', 'Doctor': 'NOUN', 'Dolittle': 'NOUN', '410': 'NUM', 'Dutch': 'NOUN', '411': 'NUM', 'Eames': 'NOUN', 'Architect': 'NOUN', 'Painter': 'NOUN', '412': 'NUM', 'Edge': 'NOUN', '413': 'NUM', 'Englishman': 'NOUN', 'Went': 'NOUN', 'Up': 'ADP', 'But': 'CONJ', 'Came': 'NOUN', 'Mountain': 'NOUN', '414': 'NUM', 'Experts': 'NOUN', '415': 'NUM', 'Wives': 'VERB', 'Club': 'NOUN', '416': 'NUM', 'Funny': 'NOUN', 'Face': 'NOUN', '417': 'NUM', 'Gifted': 'NOUN', 'Hands': 'NOUN', 'Ben': 'NOUN', 'Carson': 'NOUN', 'Story': 'NOUN', '418': 'NUM', 'Grand': 'NOUN', 'Canyon': 'NOUN', '419': 'NUM', 'Half': 'NOUN', 'Light': 'NOUN', '420': 'NUM', 'Happy-Go-Lucky': 'ADJ', '421': 'NUM', 'Harmonists': 'NOUN', '422': 'NUM', 'Heartburn': 'NOUN', '423': 'NUM', 'Le': 'NOUN', 'hérisson': 'NOUN', '424': 'NUM', 'How': 'ADV', 'About': 'ADP', '425': 'NUM', 'Idiots': 'NOUN', 'Angels': 'NOUN', '426': 'NUM', 'Impostor': 'NOUN', '427': 'NUM', 'Ink': 'NOUN', '428': 'NUM', 'Sleep': 'NOUN', '429': 'NUM', '430': 'NUM', 'Janie': 'NOUN', 'Jones': 'NOUN', '431': 'NUM', 'Jean-Michel': 'ADJ', 'Radiant': 'NOUN', '432': 'NUM', 'Jeremy': 'NOUN', 'Meaning': 'NOUN', '433': 'NUM', 'Jesus': 'NOUN', '434': 'NUM', 'Kolya': 'NOUN', '435': 'NUM', 'Lady': 'NOUN', 'Vanishes': 'NOUN', '436': 'NUM', '437': 'NUM', 'Word': 'NOUN', '438': 'NUM', '439': 'NUM', 'Goodbye': 'NOUN', '440': 'NUM', 'Cried': 'VERB', '441': 'NUM', '442': 'NUM', 'Mesrine': 'NOUN', '443': 'NUM', 'Public': 'NOUN', 'Enemy': 'NOUN', '444': 'NUM', 'Mimic': 'NOUN', '445': 'NUM', 'Crack': 'NOUN', \"'d\": 'VERB', '446': 'NUM', 'Month': 'NOUN', 'by': 'ADP', 'Lake': 'NOUN', '447': 'NUM', 'Move': 'NOUN', 'Over': 'ADP', 'Darling': 'NOUN', '448': 'NUM', 'Muppet': 'NOUN', '449': 'NUM', 'Music': 'NOUN', 'Never': 'NOUN', 'Stopped': 'VERB', '450': 'NUM', 'So': 'NOUN', '451': 'NUM', 'New': 'NOUN', '452': 'NUM', 'Nine': 'NUM', '453': 'NUM', 'Odd': 'NOUN', 'Couple': 'NOUN', '454': 'NUM', 'Around': 'ADP', '455': 'NUM', 'More': 'ADJ', 'Feeling': 'NOUN', '456': 'NUM', 'Gin-iro': 'ADJ', 'no': 'DET', 'kami': 'NOUN', 'Agito': 'NOUN', '457': 'NUM', 'Pallbearer': 'NOUN', '458': 'NUM', 'Chase': 'NOUN', '459': 'NUM', 'Party': 'NOUN', 'Monster': 'NOUN', '460': 'NUM', 'Phantoms': 'NOUN', '461': 'NUM', 'Possession': 'NOUN', '462': 'NUM', 'Radioland': 'NOUN', 'Murders': 'NOUN', '463': 'NUM', 'Reality': 'NOUN', 'Bites': 'NOUN', '464': 'NUM', 'Restoration': 'NOUN', '465': 'NUM', 'Rock-A-Doodle': 'ADJ', '466': 'NUM', 'Ruby': 'NOUN', 'Cairo': 'NOUN', '467': 'NUM', 'Runaway': 'NOUN', '468': 'NUM', 'Sabrina': 'NOUN', '469': 'NUM', 'Sassy': 'ADJ', 'Pants': 'NOUN', '470': 'NUM', 'School': 'NOUN', 'Ties': 'NOUN', '471': 'NUM', 'Scrooged': 'VERB', '472': 'NUM', 'Shakiest': 'NOUN', '473': 'NUM', 'Soapdish': 'ADJ', '474': 'NUM', 'Sometimes': 'ADV', 'They': 'PRON', '475': 'NUM', 'Summer': 'ADJ', 'Rental': 'NOUN', '476': 'NUM', '477': 'NUM', 'Fever': 'NOUN', 'Pitch': 'NOUN', '478': 'NUM', 'Them': 'NOUN', '479': 'NUM', 'Might': 'VERB', 'Giants': 'NOUN', '480': 'NUM', 'To': 'PRT', 'or': 'CONJ', '481': 'NUM', 'Trees': 'NOUN', 'Lounge': 'VERB', '482': 'NUM', 'Confessions': 'NOUN', '483,12': 'NUM', 'Holding': 'VERB', '484': 'NUM', 'Undertaking': 'NOUN', 'Betty': 'NOUN', '485': 'NUM', 'Valentin': 'NOUN', '486': 'NUM', 'Valley': 'NOUN', 'Dolls': 'NOUN', '487': 'NUM', 'Bees': 'NOUN', '488': 'NUM', '7th': 'NUM', 'Street': 'NOUN', '489': 'NUM', 'Velveteen': 'NOUN', '490': 'NUM', 'Vincent': 'NOUN', 'Theo': 'NOUN', '491': 'NUM', '492': 'NUM', '493': 'NUM', 'Mark': 'NOUN', 'Twain': 'NOUN', '494': 'NUM', 'Reluctant': 'NOUN', '495': 'NUM', 'Behind': 'ADP', 'Sun': 'NOUN', '496': 'NUM', '497': 'NUM', 'Thief': 'NOUN', 'Cobbler': 'NOUN', '498': 'NUM', 'Date': 'NOUN', 'an': 'DET', 'Angel': 'NOUN', '499': 'NUM', 'Butterfly': 'NOUN'}\n",
            "Total number of Nouns: 664\n",
            "Total number of Verbs: 44\n",
            "Total number of Adjectives: 41\n",
            "Total number of Adverbs: 10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H_e4jpcYaR9"
      },
      "source": [
        "#sentence length\r\n",
        "def str_len(x):\r\n",
        "  return len(x.split(\" \"))\r\n",
        "df['sentence length']=[*map(str_len, df['Abstracts'])]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BTIInMwkjpS"
      },
      "source": [
        "# Preposition Density\r\n",
        "from collections import Counter\r\n",
        "count=[]\r\n",
        "df['Preposition']=in_counter=[x['IN'] for x in count]\r\n",
        "df['Preposition_Density']=(df['Preposition']/df['sentence length'])*100\r\n",
        "df\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}